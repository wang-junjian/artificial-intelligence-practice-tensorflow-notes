{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 均方误差MSE：\n",
    "```\n",
    "    loss = tf.reduce_mean(tf.square(y_-y))\n",
    "```\n",
    "- 反向传播训练方法：以减小loss值为优化目标\n",
    "```\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    train_step = tf.train.MomentumDescentOptimizer(learning_rate).minimize(loss)\n",
    "    train_step = tf.train.AdamDescentOptimizer(learning_rate).minimize(loss)\n",
    "```\n",
    "- 学习率（learning_rate）：决定参数每次更新的幅度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.001\n",
    "seed = 23455"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      " [[0.83494319 0.11482951]\n",
      " [0.66899751 0.46594987]\n",
      " [0.60181666 0.58838408]\n",
      " [0.31836656 0.20502072]\n",
      " [0.87043944 0.02679395]\n",
      " [0.41539811 0.43938369]\n",
      " [0.68635684 0.24833404]\n",
      " [0.97315228 0.68541849]\n",
      " [0.03081617 0.89479913]\n",
      " [0.24665715 0.28584862]\n",
      " [0.31375667 0.47718349]\n",
      " [0.56689254 0.77079148]\n",
      " [0.7321604  0.35828963]\n",
      " [0.15724842 0.94294584]\n",
      " [0.34933722 0.84634483]\n",
      " [0.50304053 0.81299619]\n",
      " [0.23869886 0.9895604 ]\n",
      " [0.4636501  0.32531094]\n",
      " [0.36510487 0.97365522]\n",
      " [0.73350238 0.83833013]\n",
      " [0.61810158 0.12580353]\n",
      " [0.59274817 0.18779828]\n",
      " [0.87150299 0.34679501]\n",
      " [0.25883219 0.50002932]\n",
      " [0.75690948 0.83429824]\n",
      " [0.29316649 0.05646578]\n",
      " [0.10409134 0.88235166]\n",
      " [0.06727785 0.57784761]\n",
      " [0.38492705 0.48384792]\n",
      " [0.69234428 0.19687348]\n",
      " [0.42783492 0.73416985]\n",
      " [0.09696069 0.04883936]]\n",
      "Y:\n",
      " [[1], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [0], [0], [1], [0], [0], [1], [1], [0], [1], [0], [1], [1], [1], [1], [1], [0], [1]]\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(seed)\n",
    "X = rng.rand(32, 2)\n",
    "Y = [[int(x0+x1<1)] for (x0, x1) in X]\n",
    "print('X:\\n', X)\n",
    "print('Y:\\n', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))\n",
    "\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.square(y-y_))\n",
    "train_step = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)\n",
    "# train_step = tf.train.MomentumOptimizer(LEARNING_RATE, 0.9).minimize(loss)\n",
    "# train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1:\n",
      " [[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]]\n",
      "w2:\n",
      " [[-0.8113182 ]\n",
      " [ 1.4845988 ]\n",
      " [ 0.06532937]]\n",
      "\n",
      "\n",
      "After 0 training step(s), loss on all data is 5.13118\n",
      "After 500 training step(s), loss on all data is 0.429111\n",
      "After 1000 training step(s), loss on all data is 0.409789\n",
      "After 1500 training step(s), loss on all data is 0.399923\n",
      "After 2000 training step(s), loss on all data is 0.394146\n",
      "After 2500 training step(s), loss on all data is 0.390597\n",
      "\n",
      "\n",
      "w1:\n",
      " [[-0.7000663   0.9136318   0.08953571]\n",
      " [-2.3402493  -0.14641267  0.58823055]]\n",
      "w2:\n",
      " [[-0.06024267]\n",
      " [ 0.91956186]\n",
      " [-0.0682071 ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('w1:\\n', sess.run(w1))\n",
    "    print('w2:\\n', sess.run(w2))\n",
    "    print('\\n')\n",
    "    \n",
    "    STEPS = 3000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*BATCH_SIZE) % 32\n",
    "        end = start + BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
    "        if i % 500 == 0:\n",
    "            total_loss = sess.run(loss, feed_dict={x: X, y_: Y})\n",
    "            print('After %d training step(s), loss on all data is %g' % (i, total_loss))\n",
    "\n",
    "    print('\\n')\n",
    "    print('w1:\\n', sess.run(w1))\n",
    "    print('w2:\\n', sess.run(w2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
